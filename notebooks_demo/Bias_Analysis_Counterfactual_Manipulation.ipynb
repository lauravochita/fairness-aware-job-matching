{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX446xHAR0GG"
   },
   "source": [
    "#Fairness-Aware Job-Matching: Bias Analysis and Counterfactual Intervention Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DUzitPbSJ4Z"
   },
   "source": [
    "#### This notebook demonstrates bias detection and mitigation in embedding-based job-matching systems using statistical analysis and counterfactual manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn6UgLCx04a5"
   },
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeo1htjoYwMC",
    "outputId": "f4ddb83c-a786-40e4-a5f9-43e1d6744e8b"
   },
   "outputs": [],
   "source": [
    "print(\"Setting up environment...\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install scikit-learn pandas numpy scipy matplotlib seaborn plotly -q\n",
    "!pip install pydantic-settings ranx faiss-cpu -q\n",
    "\n",
    "# Mount Google Drive and setup paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add framework to path\n",
    "sys.path.append('/content/drive/MyDrive/Thesis/fairness_aware_job_matching/')\n",
    "\n",
    "# Import framework components\n",
    "from fairness_framework import (\n",
    "    Config,\n",
    "    load_embeddings,\n",
    "    load_oracle_gender_labels,\n",
    "    load_test_job_data,\n",
    "    load_resume_corpus_mapping,\n",
    "    extract_valid_job_test_indices,\n",
    "    run_bias_analysis,\n",
    "    test_representation_significance,\n",
    "    comprehensive_statistical_validation,\n",
    "    comprehensive_validation_suite,\n",
    "    train_gender_classifier_with_teacher_params,\n",
    "    analyze_gender_directions,\n",
    "    flip_embeddings_along_gender_direction,\n",
    "    run_counterfactual_bias_analysis,\n",
    "    compare_bias_results\n",
    ")\n",
    "\n",
    "def set_thesis_style():\n",
    "    sns.set(style='whitegrid', font_scale=1.1)\n",
    "    plt.rcParams.update({\n",
    "        'axes.edgecolor': 'black',\n",
    "        'axes.linewidth': 1.0,\n",
    "        'figure.dpi': 100,\n",
    "        'savefig.bbox': 'tight',\n",
    "        'legend.frameon': True,\n",
    "        'legend.loc': 'best'\n",
    "    })\n",
    "\n",
    "set_thesis_style()\n",
    "\n",
    "print(\"Environment setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiupqvVCTGJV"
   },
   "source": [
    "Data Loading & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Pc_0o2qSmeb",
    "outputId": "4e5351e1-0a16-4025-9340-276af0ad726d"
   },
   "outputs": [],
   "source": [
    "# Update configuration\n",
    "Config.update_base_path(\"/content/drive/MyDrive/Thesis/\")\n",
    "Config.PROPRIETARY_SRC_PATH = \"/content/drive/MyDrive/Thesis/fine_tuning/src\"\n",
    "Config.PROPRIETARY_INFERENCE_PATH = \"/content/drive/MyDrive/Thesis/fine_tuning/inference_endpoint/src\"\n",
    "\n",
    "# Add proprietary paths\n",
    "sys.path.append(Config.PROPRIETARY_SRC_PATH)\n",
    "sys.path.append(Config.PROPRIETARY_INFERENCE_PATH)\n",
    "\n",
    "# Load core data components\n",
    "gender_mapping = load_oracle_gender_labels()\n",
    "test_job_queries, test_job_query_ids = load_test_job_data()\n",
    "resume_id_to_text, resume_corpus_ids = load_resume_corpus_mapping()\n",
    "valid_job_indices, valid_job_query_ids = extract_valid_job_test_indices(\n",
    "    test_job_queries, (100000,)\n",
    ")\n",
    "\n",
    "# Load proprietary FAISS components\n",
    "from fairness_framework.utils.proprietary_imports import get_proprietary_components\n",
    "proprietary_components = get_proprietary_components()\n",
    "\n",
    "if proprietary_components:\n",
    "    create_faiss_index = proprietary_components['create_faiss_index']\n",
    "    search_faiss = proprietary_components['search_faiss']\n",
    "    print(\"FAISS search components loaded successfully\")\n",
    "else:\n",
    "    raise RuntimeError(\"Could not load required search components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjHdzjPRTb6Z"
   },
   "source": [
    "### 1.Bias Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "d968aa6887c34d31a12829bef285e5ed",
      "f446d2b9736d40eaba661944d2e2911e",
      "42b9d5e2fec94b719bcdb1bc4625e198",
      "8f1924d2981544c3b0f4a91342618e8b",
      "ccd1c3a1758c4e3492fd24ce5baa1223",
      "5d1fdc30bf0d427197f01fe13ff285d1",
      "9800142804a44c719a0b5fdde39b44e7",
      "b179123143764c7cb0e41849ad018c85",
      "97529abecdcb4dff80d09b5c795df7ee",
      "68e48aff248641db9264581c9aebc9a3",
      "bcb1ff19d0d148dd8748f253985fff6d"
     ]
    },
    "id": "CxlkfkSdTQcp",
    "outputId": "ed5253ca-a6c1-4abe-d7c7-a73e22e9083e"
   },
   "outputs": [],
   "source": [
    "def run_jobs_to_resumes_analysis(modified_embeddings=None):\n",
    "    \"\"\"Execute bias analysis using the test job queries\"\"\"\n",
    "    return run_bias_analysis(\n",
    "        test_job_queries=test_job_queries,\n",
    "        gender_mapping=gender_mapping,\n",
    "        resume_corpus_ids=resume_corpus_ids,\n",
    "        create_faiss_index=create_faiss_index,\n",
    "        search_faiss=search_faiss,\n",
    "        modified_cv_embeddings=modified_embeddings\n",
    "    )\n",
    "\n",
    "print(\"\\nRunning comprehensive bias analysis...\")\n",
    "original_results = run_jobs_to_resumes_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAa0j7VXTkmW",
    "outputId": "439e25ec-6636-4938-b851-3b404d411f1e"
   },
   "outputs": [],
   "source": [
    "print(\"\\nSimilarity Score Distribution Analysis\")\n",
    "if 'score_distribution' in original_results:\n",
    "    score_stats = original_results['score_distribution']\n",
    "\n",
    "    # Display score statistics\n",
    "    results_table = []\n",
    "    for gender, stats in score_stats.items():\n",
    "        if isinstance(stats, dict) and 'mean' in stats:\n",
    "            results_table.append({\n",
    "                'Gender': gender.capitalize(),\n",
    "                'Count': f\"{stats['count']:,}\",\n",
    "                'Mean': f\"{stats['mean']:.4f}\",\n",
    "                'Std': f\"{stats['std']:.4f}\",\n",
    "                'Median': f\"{stats['median']:.4f}\",\n",
    "                'Q25': f\"{stats['percentile_25']:.4f}\",\n",
    "                'Q75': f\"{stats['percentile_75']:.4f}\"\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results_table)\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # Statistical significance test\n",
    "    if 'statistical_test' in score_stats:\n",
    "        test = score_stats['statistical_test']\n",
    "        print(f\"\\nStatistical Test Results:\")\n",
    "        print(f\"  T-statistic: {test['t_statistic']:.4f}\")\n",
    "        print(f\"  P-value: {test['p_value']:.4f}\")\n",
    "        print(f\"  Effect size (Cohen's d): {test['effect_size']:.4f}\")\n",
    "        print(f\"  Significant difference: {test['significant']}\")\n",
    "\n",
    "        if test['significant']:\n",
    "            advantage_gender = \"Female\" if test['effect_size'] > 0 else \"Male\"\n",
    "            effect_magnitude = \"Large\" if abs(test['effect_size']) >= 0.8 else \"Medium\" if abs(test['effect_size']) >= 0.5 else \"Small\"\n",
    "            print(f\"  Result: {advantage_gender} candidates show {effect_magnitude.lower()} score advantage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hV0l0z8OTvWE",
    "outputId": "c44dc98e-5702-4872-cf4f-7927b48519e2"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTop-K Representation Analysis\")\n",
    "if 'top_k_representation' in original_results:\n",
    "    repr_stats = original_results['top_k_representation']\n",
    "    population_props = Config.POPULATION_PROPORTIONS\n",
    "\n",
    "    representation_data = []\n",
    "\n",
    "    for k, data in repr_stats.items():\n",
    "        total_positions = sum(stats['count'] for stats in data.values())\n",
    "\n",
    "        for gender, stats in data.items():\n",
    "            expected_pct = population_props.get(gender, 0) * 100\n",
    "            actual_pct = stats['percentage']\n",
    "            difference = actual_pct - expected_pct\n",
    "\n",
    "            representation_data.append({\n",
    "                'Ranking': k.replace('_', '-').upper(),\n",
    "                'Gender': gender.capitalize(),\n",
    "                'Count': f\"{stats['count']:,}\",\n",
    "                'Actual%': f\"{actual_pct:.1f}%\",\n",
    "                'Expected%': f\"{expected_pct:.1f}%\",\n",
    "                'Difference': f\"{difference:+.1f}%\"\n",
    "            })\n",
    "\n",
    "    repr_df = pd.DataFrame(representation_data)\n",
    "    print(repr_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXZjr8cvTzjD",
    "outputId": "c1d4282f-6caf-495d-a1ac-9575b6e8ce4a"
   },
   "outputs": [],
   "source": [
    "print(\"\\nRepresentation Significance Testing\")\n",
    "representation_test_results = test_representation_significance(\n",
    "    search_results=original_results['search_results'],\n",
    "    gender_mapping=gender_mapping,\n",
    "    population_proportions=Config.POPULATION_PROPORTIONS,\n",
    "    analysis_name=\"Jobs to Resumes\"\n",
    ")\n",
    "\n",
    "# Summarize significance results\n",
    "sig_summary = []\n",
    "for k, results in representation_test_results.items():\n",
    "    sig_summary.append({\n",
    "        'Ranking': k.replace('_', '-').upper(),\n",
    "        'Male_pvalue': f\"{results['male_p_value']:.4f}\",\n",
    "        'Female_pvalue': f\"{results['female_p_value']:.4f}\",\n",
    "        'Male_Effect': f\"{results['male_effect_size']:+.3f}\",\n",
    "        'Female_Effect': f\"{results['female_effect_size']:+.3f}\",\n",
    "        'Chi2_pvalue': f\"{results['chi2_p_value']:.4f}\"\n",
    "    })\n",
    "\n",
    "sig_df = pd.DataFrame(sig_summary)\n",
    "print(sig_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZRNAronT-PM",
    "outputId": "c602520d-ce34-48a7-8336-6fdcf90ff503"
   },
   "outputs": [],
   "source": [
    "print(\"\\nStatistical Validation\")\n",
    "comprehensive_stats = comprehensive_statistical_validation(\n",
    "    search_results=original_results['search_results'],\n",
    "    gender_mapping=gender_mapping,\n",
    "    population_proportions=Config.POPULATION_PROPORTIONS\n",
    ")\n",
    "\n",
    "# Bootstrap validation results\n",
    "if 'bootstrap_validation' in comprehensive_stats:\n",
    "    bootstrap = comprehensive_stats['bootstrap_validation']\n",
    "    print(f\"Bootstrap Validation (1000 iterations):\")\n",
    "    print(f\"  Score advantage: {bootstrap['score_advantage_mean']:+.4f}\")\n",
    "    print(f\"  95% CI: [{bootstrap['score_advantage_ci'][0]:+.4f}, {bootstrap['score_advantage_ci'][1]:+.4f}]\")\n",
    "    print(f\"  Rank advantage: {bootstrap['rank_advantage_mean']:+.2f}\")\n",
    "    print(f\"  95% CI: [{bootstrap['rank_advantage_ci'][0]:+.2f}, {bootstrap['rank_advantage_ci'][1]:+.2f}]\")\n",
    "\n",
    "# Integrity validation\n",
    "print(f\"\\nIntegrity Validation:\")\n",
    "validation_results = comprehensive_validation_suite(\n",
    "    search_results=original_results['search_results'],\n",
    "    gender_mapping=gender_mapping,\n",
    "    sample_size=10,\n",
    "    n_spot_checks=3\n",
    ")\n",
    "\n",
    "val_summary = validation_results['validation_summary']\n",
    "print(f\"  Rankings properly sorted: {val_summary['rankings_properly_sorted']}\")\n",
    "print(f\"  Paradox detected: {val_summary['paradox_detected']}\")\n",
    "print(f\"  Queries validated: {val_summary['total_queries_validated']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaUx3_4eUWvQ"
   },
   "source": [
    "###2. Counterfactual Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caUkHq6AWMcP",
    "outputId": "ab3df248-31d7-4b12-8e5b-3196f19a2abd"
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading embeddings for intervention...\")\n",
    "\n",
    "# Load the same embeddings used in bias analysis\n",
    "from fairness_framework.data.data_loader import load_bias_analysis_embeddings\n",
    "job_embeddings, cv_embeddings = load_bias_analysis_embeddings()\n",
    "\n",
    "print(f\"Loaded embeddings: jobs={job_embeddings.shape}, cvs={cv_embeddings.shape}\")\n",
    "\n",
    "# Use the same valid job indices that were used in bias analysis\n",
    "job_embeddings_subset = job_embeddings[valid_job_indices]\n",
    "print(f\"Using subset of job embeddings: {job_embeddings_subset.shape}\")\n",
    "\n",
    "# The gender mapping uses resume IDs, so we need to map those to CV embedding indices\n",
    "resume_indices_with_gender = []\n",
    "gender_labels = []\n",
    "\n",
    "# Map resume IDs from gender_mapping to CV embedding indices\n",
    "for resume_id, gender in gender_mapping.items():\n",
    "    try:\n",
    "        # Extract index from resume_id (format: \"resume_X\")\n",
    "        index = int(resume_id.split('_')[1])\n",
    "        if index < cv_embeddings.shape[0]:\n",
    "            resume_indices_with_gender.append(index)\n",
    "            gender_labels.append(0 if gender == \"female\" else 1)\n",
    "    except (ValueError, IndexError):\n",
    "        continue\n",
    "\n",
    "gender_labels = np.array(gender_labels)\n",
    "labeled_embeddings = cv_embeddings[resume_indices_with_gender]\n",
    "\n",
    "print(f\"Labeled embeddings: {len(gender_labels)} samples\")\n",
    "print(f\"Gender distribution: Female={np.sum(gender_labels == 0)}, Male={np.sum(gender_labels == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClipsU0JUpVZ",
    "outputId": "b6cf4e87-25fc-400a-be7c-252d15fa8dfd"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining Gender Classifier\")\n",
    "classifier, cv_accuracy, train_accuracy = train_gender_classifier_with_teacher_params(\n",
    "    labeled_embeddings, gender_labels\n",
    ")\n",
    "\n",
    "print(f\"Classifier Performance:\")\n",
    "print(f\"  Cross-validation accuracy: {cv_accuracy:.4f}\")\n",
    "print(f\"  Training accuracy: {train_accuracy:.4f}\")\n",
    "# Analyze gender direction\n",
    "gender_weights, direction_norm = analyze_gender_directions(classifier, labeled_embeddings)\n",
    "\n",
    "print(f\"\\nGender Direction Analysis:\")\n",
    "print(f\"  Direction norm: {direction_norm:.6f}\")\n",
    "print(f\"  Weight statistics: mean={np.mean(gender_weights):.6f}, std={np.std(gender_weights):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "af62e5eac46948c088cf0c0a5e2b44fe",
      "e5db1284ae874bdda2c029dd813c4508",
      "9de4bc88cdcf44d39a2afdf450516b34",
      "016516242e4648059d7b938e2ab8b580",
      "5128f6f03c404d80a655591b7e5c4393",
      "3dee985daa1d427b95a8299096125b61",
      "d096a7c221864b739fa9d2160e7aa105",
      "99799405c6d04a8d8510ca8a94806ef7",
      "c55117b1e0a34435bd5cc57f77b4ed45",
      "8866306820fa4b19b831ab2f82f18d47",
      "e6cf13d7c7be46d3944352780875b201",
      "f86ebc41b5fa45c1936e0cf1bb39e8b5",
      "086d1421f7a640db9a7dcb63e689be32",
      "2074bc6387dd4da985d693ace8d39b61",
      "4044e536b39142bba320ef6837e37a2f",
      "de921cc084b84c89928e98163bda4ea3",
      "e02550a394d54682b8662eb7b2d70ca2",
      "9d92cd0d7be14783afddb4b925993f50",
      "947800ec144d4d5a8391c55ee801b60c",
      "0acda228e5694be0b64ba695a5bc4aa5",
      "9b19473cf9304961be83367bf374a510",
      "d9eeaddd89c74acd8ab5a4a3e891c2d9",
      "0357445a33c740d7ba4ed34493ae95fa",
      "243ee7947e5a4393898e7498fd9fbf20",
      "def2084b155e49d9becd7941a81846dc",
      "3ca97d63d2cc47f8ad3632c06f825d94",
      "b2f356c8445b4c5492af17e684b92f92",
      "7e2f7ae0c52b49989a70226f9b95433d",
      "c869c93306eb474490ba5df0a9d28f0b",
      "92f57cf2ee484feab167fa9789a5f1f2",
      "4c59e8b5a68f411aa5eaaf9573d000aa",
      "0c86d6ebe2894673b4793e09f02170c6",
      "8714c99aac72480e9a8b330e8b38d87e",
      "407aaf669f4b4374968d801ea169d6f2",
      "524990e328e545c7b8e322cc98dd6316",
      "9794d7f15fab4a4cbe4a04e597e220f7",
      "ff70164193ef49ba8f01dd18c977db9b",
      "cd84ca09c63f421dbaa7b99ec3599d96",
      "38d94eadb4d348e9b0a96c886b056c0f",
      "404c61bbaffd43e9b5b17aa667cb1904",
      "81b88e68f0fc45bdb01a8ad7cef2da1c",
      "5c6398b8697e4c09b3f996ef37c75588",
      "eabd952cb2114f1ea97296f1364df509",
      "802c543cf61a48009b34da106773deb2"
     ]
    },
    "id": "gLxkCDYrZd_Q",
    "outputId": "e3e0e067-9460-43b8-fa35-1a7cdb852b29"
   },
   "outputs": [],
   "source": [
    "print(\"Counterfactual Intervention Analysis\")\n",
    "print(\"\\nLoading embeddings for intervention...\")\n",
    "print(\"\\nRunning Counterfactual Analysis\")\n",
    "# Use the modular counterfactual analysis function\n",
    "counterfactual_results = run_counterfactual_bias_analysis(\n",
    "    gender_mapping=gender_mapping,\n",
    "    run_bias_analysis_jobs_to_resumes_func=run_jobs_to_resumes_analysis\n",
    ")\n",
    "\n",
    "if counterfactual_results is None:\n",
    "    print(\"Counterfactual analysis failed\")\n",
    "else:\n",
    "    print(\"Counterfactual analysis completed successfully\")\n",
    "\n",
    "    # Display results summary\n",
    "    print(f\"\\nGender Classifier Performance:\")\n",
    "    print(f\"  CV accuracy: {counterfactual_results['original_cv_acc']:.4f}\")\n",
    "    print(f\"  Training accuracy: {counterfactual_results['original_train_acc']:.4f}\")\n",
    "\n",
    "    # Display intervention results\n",
    "    print(f\"\\nIntervention Results:\")\n",
    "    intervention_results = []\n",
    "\n",
    "    for factor, results in counterfactual_results[\"flip_results\"].items():\n",
    "        accuracy_drop = results['classifier_accuracy_drop']\n",
    "\n",
    "        # Extract bias metrics\n",
    "        if 'jobs_to_resumes' in results and 'score_distribution' in results['jobs_to_resumes']:\n",
    "            modified_stats = results['jobs_to_resumes']['score_distribution']\n",
    "            if 'statistical_test' in modified_stats:\n",
    "                modified_test = modified_stats['statistical_test']\n",
    "                original_test = original_results['score_distribution']['statistical_test']\n",
    "\n",
    "                bias_reduction = abs(original_test['effect_size']) - abs(modified_test['effect_size'])\n",
    "\n",
    "                print(f\"\\nFlip factor α = {factor}\")\n",
    "                print(f\"  Classifier accuracy drop: {accuracy_drop:.4f}\")\n",
    "                print(f\"  Effect size change: {original_test['effect_size']:.4f} → {modified_test['effect_size']:.4f}\")\n",
    "                print(f\"  Bias reduction: {bias_reduction:.4f}\")\n",
    "                print(f\"  P-value change: {original_test['p_value']:.4f} → {modified_test['p_value']:.4f}\")\n",
    "\n",
    "                intervention_results.append({\n",
    "                    'Flip_Factor': factor,\n",
    "                    'Accuracy_Drop': accuracy_drop,\n",
    "                    'Original_Effect': original_test['effect_size'],\n",
    "                    'Modified_Effect': modified_test['effect_size'],\n",
    "                    'Bias_Reduction': bias_reduction,\n",
    "                    'Original_Pvalue': original_test['p_value'],\n",
    "                    'Modified_Pvalue': modified_test['p_value']\n",
    "                })\n",
    "\n",
    "    # Use modular comparison function for detailed analysis\n",
    "    print(f\"\\nDetailed Comparative Analysis:\")\n",
    "    compare_bias_results(original_results, counterfactual_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fqh_pM9mhZWf",
    "outputId": "5dc6a260-bb96-475a-a6c9-ab9311c0f15d"
   },
   "outputs": [],
   "source": [
    "print(\"\\nIntervention Effectiveness Summary\")\n",
    "if intervention_results:\n",
    "    intervention_df = pd.DataFrame(intervention_results)\n",
    "\n",
    "    # Format for display\n",
    "    display_df = intervention_df.copy()\n",
    "    for col in ['Accuracy_Drop', 'Original_Effect', 'Modified_Effect', 'Bias_Reduction']:\n",
    "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "    for col in ['Original_Pvalue', 'Modified_Pvalue']:\n",
    "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "    print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T6ITtHnJkDK1",
    "outputId": "0d5dfdcb-30ea-4195-9ff9-23c4d8dab94a"
   },
   "outputs": [],
   "source": [
    "print(\"RESULTS SUMMARY\")\n",
    "# Compile key findings\n",
    "print(\"\\nKey Findings:\")\n",
    "\n",
    "# Score analysis summary\n",
    "if 'score_distribution' in original_results and 'statistical_test' in original_results['score_distribution']:\n",
    "    test = original_results['score_distribution']['statistical_test']\n",
    "    print(f\"1. Similarity Score Analysis:\")\n",
    "    print(f\"   - Effect size: {test['effect_size']:.4f}\")\n",
    "    print(f\"   - P-value: {test['p_value']:.4f}\")\n",
    "    print(f\"   - Statistical significance: {test['significant']}\")\n",
    "\n",
    "# Representation summary\n",
    "if 'top_k_representation' in original_results and 'top_5' in original_results['top_k_representation']:\n",
    "    top5_data = original_results['top_k_representation']['top_5']\n",
    "    population_props = Config.POPULATION_PROPORTIONS\n",
    "\n",
    "    print(f\"2. Representation Analysis (Top-5):\")\n",
    "    for gender in ['female', 'male']:\n",
    "        if gender in top5_data:\n",
    "            actual = top5_data[gender]['percentage']\n",
    "            expected = population_props[gender] * 100\n",
    "            print(f\"   - {gender.capitalize()}: {actual:.1f}% (expected: {expected:.1f}%)\")\n",
    "\n",
    "# Intervention summary\n",
    "if intervention_results:\n",
    "    max_reduction = max(r['Bias_Reduction'] for r in intervention_results)\n",
    "    print(f\"3. Intervention Effectiveness:\")\n",
    "    print(f\"   - Maximum bias reduction: {max_reduction:.4f}\")\n",
    "    print(f\"   - Interventions tested: {len(intervention_results)}\")\n",
    "\n",
    "print(f\"\\nAnalysis completed.\")\n",
    "print(f\"Total queries analyzed: {len(original_results.get('search_results', {}))}\")\n",
    "print(f\"Candidates evaluated: {len(gender_mapping)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
